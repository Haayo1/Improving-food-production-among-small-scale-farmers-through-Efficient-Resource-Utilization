---
title: "Business Intelligence Project"
author: "<GroundBreakers>"
date: "<26/11/2023>"
output:
  github_document: 
    toc: yes
    toc_depth: 4
    fig_width: 6
    fig_height: 4
    df_print: default
editor_options:
  chunk_output_type: console
---

# Student Details

+----------------------------------------------+-----------------------+
| **Student ID Number**                        | 134111                |
|                                              |                       |
|                                              | 120313                |
|                                              |                       |
|                                              |                       |
|                                              |                       |
|                                              |                       |
|                                              |                       |
|                                              |                       |
+----------------------------------------------+-----------------------+
| **Student Name**                             | Juma Immaculate Haayo |
|                                              |                       |
|                                              | Daniel Obala          |
|                                              |                       |
|                                              |                       |
|                                              |                       |
|                                              |                       |
|                                              |                       |
|                                              |                       |
+----------------------------------------------+-----------------------+
| **BBIT 4.2 Group**                           | BBIT Exempt           |
+----------------------------------------------+-----------------------+
| **BI Project Group Name/ID (if applicable)** | GroundBreakers        |
+----------------------------------------------+-----------------------+

# Setup Chunk

**Note:** the following KnitR options have been set as the global defaults: <BR> `knitr::opts_chunk$set(echo = TRUE, warning = FALSE, eval = TRUE, collapse = FALSE, tidy = TRUE)`.

More KnitR options are documented here <https://bookdown.org/yihui/rmarkdown-cookbook/chunk-options.html> and here <https://yihui.org/knitr/options/>.

```{r setup, include=FALSE}
library(formatR)
knitr::opts_chunk$set(
  warning = FALSE,
  collapse = FALSE
)
```

# Understanding the Dataset (Exploratory Data Analysis (EDA))

## Downloading the Dataset

### Source:

The dataset that was used can be downloaded here: *\<<https://www.kaggle.com/code/triptmann/irrigation-scheduling-for-smart-agriculture/input>\>*

### Reference:

*\<Cite the dataset here using APA\>\
Refer to the APA 7th edition manual for rules on how to cite datasets: <https://apastyle.apa.org/style-grammar-guidelines/references/examples/data-set-references>*

# Install and load all the packages
We installed all the packages that will enable us execute our project.

```{r Your 1st Code Chunk}
# Install renv:
if (!is.element("renv", installed.packages()[, 1])) {
  install.packages("renv", dependencies = TRUE)
}
require("renv")

## dplyr ----
if (!is.element("dplyr", installed.packages()[, 1])) {
  install.packages("dplyr", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}
require("dplyr")

## naniar ----
# Documentation:
#   https://cran.r-project.org/package=naniar or
#   https://www.rdocumentation.org/packages/naniar/versions/1.0.0
if (!is.element("naniar", installed.packages()[, 1])) {
  install.packages("naniar", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}
require("naniar")

## ggplot2 ----
# We require the "ggplot2" package to create more appealing visualizations
if (!is.element("ggplot2", installed.packages()[, 1])) {
  install.packages("ggplot2", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}
require("ggplot2")

## MICE ----
# We use the MICE package to perform data imputation
if (!is.element("mice", installed.packages()[, 1])) {
  install.packages("mice", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}
require("mice")

## Amelia ----
if (!is.element("Amelia", installed.packages()[, 1])) {
  install.packages("Amelia", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}
require("Amelia")


## MICE ----
# We use the MICE package to perform data imputation
if (!is.element("mice", installed.packages()[, 1])) {
  install.packages("mice", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}
require("mice")

if (!is.element("e1071", installed.packages()[, 1])) {
  install.packages("e1071", dependencies = TRUE)
}
require("e1071")

if (!is.element("ggcorrplot", installed.packages()[, 1])) {
  install.packages("ggcorrplot", dependencies = TRUE)
}
require("ggcorrplot")

## caret ----
if (require("caret")) {
  require("caret")
} else {
  install.packages("caret", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## klaR ----
if (require("klaR")) {
  require("klaR")
} else {
  install.packages("klaR", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}


## readr ----
if (require("readr")) {
  require("readr")
} else {
  install.packages("readr", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## LiblineaR ----
if (require("LiblineaR")) {
  require("LiblineaR")
} else {
  install.packages("LiblineaR", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## naivebayes ----
if (require("naivebayes")) {
  require("naivebayes")
} else {
  install.packages("naivebayes", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## stats ----
if (require("stats")) {
  require("stats")
} else {
  install.packages("stats", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## mlbench ----
if (require("mlbench")) {
  require("mlbench")
} else {
  install.packages("mlbench", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}


## MASS ----
if (require("MASS")) {
  require("MASS")
} else {
  install.packages("MASS", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## glmnet ----
if (require("glmnet")) {
  require("glmnet")
} else {
  install.packages("glmnet", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}


## kernlab ----
if (require("kernlab")) {
  require("kernlab")
} else {
  install.packages("kernlab", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## rpart ----
if (require("rpart")) {
  require("rpart")
} else {
  install.packages("rpart", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## randomForest ----
if (require("randomForest")) {
  require("randomForest")
} else {
  install.packages("randomForest", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## RRF ----
if (require("RRF")) {
  require("RRF")
} else {
  install.packages("RRF", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## caretEnsemble ----
if (require("caretEnsemble")) {
  require("caretEnsemble")
} else {
  install.packages("caretEnsemble", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## C50 ----
if (require("C50")) {
  require("C50")
} else {
  install.packages("C50", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## adabag ----
if (require("adabag")) {
  require("adabag")
} else {
  install.packages("adabag", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## plumber ----
if (require("plumber")) {
  require("plumber")
} else {
  install.packages("plumber", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## httr ----
if (require("httr")) {
  require("httr")
} else {
  install.packages("httr", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## jsonlite ----
if (require("jsonlite")) {
  require("jsonlite")
} else {
  install.packages("jsonlite", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

```

# Load datasets
## DATASET: Intelligent Irrigation System
### Description 

Our system's primary function is to provide automatic water supply. Farmers are pleased with the autonomous water supply based on their crops, according to the product experience. Product functionalities include providing water as needed and automatically activating the relay motor. Real-time sensing and control, self-control, and complete removal of man power are product features. Farmers who can autonomously supply water to different crops based on their needs are examples of customer revalidation.This proposal offered an embedded system for autonomous irrigation control. When you go on vacation, you can water your plants on a regular basis. A wireless sensor network is used in the system for real-time sensing and control of an irrigation system. This system offers regular and required water levels for the garden and farm while minimizing water waste. This method is meant to build an automatic irrigation mechanism that detects the moisture content of the earth and turns the pumping motor ON and OFF. In this setup, we connect the Arduino board to the soil moisture sensor and the dht11 sensor.The primary goal of this project is to create an intelligent irrigation system that analyzes soil moisture and assists in deciding whether to turn on or off the water supply. The goal of this project is to offer an autonomous irrigation system for plants, which will aid in water conservation. The primary goal of this initiative is to eliminate human labor while also conserving water and the environment.

```{r Your 2nd Code Chunk}
library(readr)
IrrigationScheduling <- read_csv("../data/IrrigationScheduling.csv")
View(IrrigationScheduling)

```

# Exploratory Data Analysis

The code below is used to initialize renv in a new or existing project

The prompt received after executing renv::init() is as shown below:
This project already has a lockfile. What would you like to do?
1: Restore the project from the lockfile.
2: Discard the lockfile and re-initialize the project.
3: Activate the project without snapshotting or installing any packages.
4: Abort project initialization.

Then you proceed to chose the first option
```{r Your 3rd Code Chunk}
renv::init()

```

##Dimensions
Refer to the number of observations (rows) and the number of attributes/variables/features (columns).

```{r Your 4th Code Chunk}
dim(IrrigationScheduling)

```

## Identify the Data Types 

Knowing the data types will help you to identify the most appropriate visualization types and algorithms that can be applied.
It can also help you to identify the need to convert from categorical data (factors) to integers
or vice versa where necessary

```{r Your 5th Code Chunk}
sapply(IrrigationScheduling, class)

```

## Descriptive Statistics 

### Measures of Frequency 
Identify the number of instances that belong to each class
It is more sensible to count categorical variables (factors or dimensions) than numeric variables

```{r Your 6th Code Chunk}
irrigation_scheduling_freq <- IrrigationScheduling$class
cbind(frequency = table(irrigation_scheduling_freq),
      percentage = prop.table(table(irrigation_scheduling_freq)) * 100)

```

### Measures of Central Tendency

#### Calculate the mode
We identify the class that appears the most

```{r Your 7th Code Chunk}
irrigation_scheduling_chas_mode <- names(table(IrrigationScheduling$class))[
  which(table(IrrigationScheduling$class) == max(table(IrrigationScheduling$class)))
]
print(irrigation_scheduling_chas_mode)

```

## Measures of Distribution/Dispersion/Spread/Scatter/Variability 

### Measure the distribution of the data for each variable
```{r Your 8th Code Chunk}
summary(IrrigationScheduling)

```

### Measure the standard deviation of each variable 
Low variability is ideal because it means that you can better predict information about the population based on sample data. High variability means that the values are less consistent, thus making it harder to make predictions.

```{r Your 9th Code Chunk}
sapply(IrrigationScheduling[, 2:3], sd)

```

###  Measure the variance of each variable

```{r Your 10th Code Chunk}
sapply(IrrigationScheduling[, 2:3], var)

```

### Measure the kurtosis of each variable

Informs you of how often outliers occur in the results.

```{r Your 11th Code Chunk}
sapply(IrrigationScheduling[, 2:3],  kurtosis, type = 2)

```

## Measures of Relationship

### Measure the covariance between variables 

```{r Your 12th Code Chunk}
irrigation_scheduling_cov <- cov(IrrigationScheduling[, 2:3])
View(irrigation_scheduling_cov)

```


### Perform ANOVA on the “crop_dataset” dataset 
Estimate how a quantitative dependent variable changes according to the levels of one or
more categorical independent variables.

The code below performs two way anova whereby we are testing the effect of temperature and and soilmiosture on altitude

```{r Your 13th Code Chunk}
irrigation_scheduling_additive_two_way_anova <- aov(altitude ~temperature +soilmiosture, # nolint
                                           data = IrrigationScheduling)
summary(irrigation_scheduling_additive_two_way_anova)

```

## Univariate Plots 

### Create Box and Whisker Plots for Each Numeric Attribute 

```{r Your 14th Code Chunk}
boxplot(IrrigationScheduling[, 5], main = names(IrrigationScheduling)[5])

```

### Create Bar Plots for Each Categorical Attribute
This is done using a bar chart to give an idea of the proportion of instances that belong to each category.

```{r Your 15th Code Chunk}
barplot(table(IrrigationScheduling[, 7]), main = names(IrrigationScheduling)[7])

```

### Create a Missingness Map to Identify Missing Data
Horizontal lines indicate missing data for an instance whereas vertical lines
indicate missing data for an attribute.

```{r Your 16th Code Chunk}
missmap(IrrigationScheduling, col = c("red", "grey"), legend = TRUE)

```


## Multivariate Plots 

### Create a ggcorplot
ggcorplot function helps in making a more appealing graph

```{r Your 17th Code Chunk}
ggcorrplot(cor(IrrigationScheduling[, 1:3]))

```

# Data Imputation

## Confirm the 'missingness' in the dataset before Imputation 
We are checking if there are any missing values in the Irrigation Scheduling dataset

```{r Your 18th Code Chunk}
any_na(IrrigationScheduling)

```

## How many?
We are finding the number of missing values in the Irrigation Scheduling dataset

```{r Your 19th Code Chunk}
n_miss(IrrigationScheduling)

```

## What is the percentage of missing data in the entire dataset?
We are finding the percentage of missing values in the Irrigation Scheduling dataset

```{r Your 20th Code Chunk}
prop_miss(IrrigationScheduling)

```

## How many missing values does each variable have?
We are finding the missing values in the  each variable 

```{r Your 21st Code Chunk}
IrrigationScheduling %>% is.na() %>% colSums()

```

## What is the number and percentage of missing values grouped by each variable?

```{r Your 22nd Code Chunk}

miss_var_summary(IrrigationScheduling)

```

## What is the number and percentage of missing values grouped by each observation?

```{r Your 23rd Code Chunk}
miss_case_summary(IrrigationScheduling)

```

## Which variables contain the most missing values?

```{r Your 24th Code Chunk}
gg_miss_var(IrrigationScheduling)

```

## Where are missing values located (the shaded regions in the plot)?

```{r Your 25th Code Chunk}
vis_miss(IrrigationScheduling) + theme(axis.text.x = element_text(angle = 80))

```



## Create the visualization

```{r Your 26th Code Chunk}
gg_miss_fct(IrrigationScheduling, fct = altitude)

```

## Removing the variables that are irrelevant
We remove the date and time variables because we do not need them

```{r Your 27th Code Chunk}
IrrigationScheduling <-IrrigationScheduling[-9:-10]

```

## Use the MICE package to perform data imputation

```{r Your 28th Code Chunk}
somewhat_correlated_variables_std3 <- quickpred(IrrigationScheduling, mincor = 0.3) # nolint


irrigation_scheduling_mice <- mice(IrrigationScheduling, m = 11, method = "pmm",
                             seed = 7,
                             predictorMatrix = somewhat_correlated_variables_std3)

```

## Impute the missing data

We then create imputed data for the dataset using the mice::complete()
function in the mice package to fill in the missing data.

```{r Your 29th Code Chunk}
irrigation_scheduling_imputed <- mice::complete(irrigation_scheduling_mice, 1)

```

## Confirm the "missingness" in the Imputed Dataset
A textual confirmation that the dataset has no more missing values in any feature
```{r Your 30th Code Chunk}
miss_var_summary(irrigation_scheduling_imputed)

```

## A visual confirmation that the dataset has no more missing values in any feature
```{r Your 31st Code Chunk}
Amelia::missmap(irrigation_scheduling_imputed)

```

# Repeating Data Imputation after finding missing data in variable altitude

## Are there missing values in the dataset?

```{r Your 32nd Code Chunk}
any_na(irrigation_scheduling_imputed)

```

## How many?
```{r Your 33rd Code Chunk}
n_miss(irrigation_scheduling_imputed)

```

## What is the percentage of missing data in the entire dataset?
```{r Your 34th Code Chunk}
prop_miss(irrigation_scheduling_imputed)

```

## How many missing values does each variable have?
```{r Your 35th Code Chunk}
irrigation_scheduling_imputed %>% is.na() %>% colSums()

```

## What is the number and percentage of missing values grouped by each variable?
```{r Your 36th Code Chunk}
miss_var_summary(irrigation_scheduling_imputed)

```

##What is the number and percentage of missing values grouped by each observation?
```{r Your 37th Code Chunk}
miss_case_summary(irrigation_scheduling_imputed)

```

## Which variables contain the most missing values?
```{r Your 38th Code Chunk}
gg_miss_var(irrigation_scheduling_imputed)
```

## Where are missing values located (the shaded regions in the plot)?
```{r Your 39th Code Chunk}

vis_miss(irrigation_scheduling_imputed) + theme(axis.text.x = element_text(angle = 80))
```

## Create the visualization
```{r Your 40th Code Chunk}

vis_miss(irrigation_scheduling_imputed) + theme(axis.text.x = element_text(angle = 80))
```

# Resampling Methods

## Perform str() function
Used to compactly display the structure (variables and data types) of the dataset
```{r Your 41st Code Chunk}
str(irrigation_scheduling_imputed)

```

## Split the dataset
Define a 80:20 train:test data split of the dataset.
That is, 80% of the original data will be used to train the model and
20% of the original data will be used to test the model.

```{r Your 42nd Code Chunk}
train_index <- createDataPartition(irrigation_scheduling_imputed$class,
                                   p = 0.8,
                                   list = FALSE)
IrrigationScheduling_train <- irrigation_scheduling_imputed[train_index, ]
IrrigationScheduling_test <- irrigation_scheduling_imputed[-train_index, ]

```

# :Bootstrapping 

### Bootstrapping train control
Allows you to specify that bootstrapping (sampling with
replacement) can be used and also the number of times (repetitions or reps)the sampling with replacement should be done.

This increases the size of the training dataset from 60 observations to
approximately 60 x 40 = 2,400 observations for training the model.

```{r Your 43rd Code Chunk}
train_control <- trainControl(method = "boot", number = 40)

```

# Algorithm Selection

# Algorithm Selection for Classification 
# Linear Algorithms 
## Linear Discriminant Analysis
### Linear Discriminant Analysis without caret 

```{r Your 44th Code Chunk}
sapply(irrigation_scheduling_imputed, class)

```

#### Train the model
```{r Your 45th Code Chunk}
IrrigationScheduling_model_lda <- lda(class ~soilmiosture, data = IrrigationScheduling_train)

```

#### Display the model's details 
```{r Your 46th Code Chunk}
print(IrrigationScheduling_model_lda)

```

#### Make predictions
```{r Your 47th Code Chunk}
predictions <- predict(IrrigationScheduling_model_lda,
                       irrigation_scheduling_imputed[, 1:5])$class

```

### Linear Discriminant Analysis with caret


#### Train the model

We apply a Leave One Out Cross Validation resampling method.
We also apply a standardize data transform to make the mean = 0 and
standard deviation = 1

```{r Your 48th Code Chunk}
set.seed(7)
train_control <- trainControl(method = "LOOCV")
IrrigationScheduling_caret_model_lda <- train(class ~ soilmiosture,
                                     data = IrrigationScheduling_train,
                                     method = "lda", metric = "Accuracy",
                                     preProcess = c("center", "scale"),
                                     trControl = train_control)
```

#### Display the model's details

```{r Your 49th Code Chunk}
print(IrrigationScheduling_caret_model_lda)
```

#### Make predictions 
```{r Your 50th Code Chunk}
predictions <- predict(IrrigationScheduling_caret_model_lda,
                       IrrigationScheduling_test[, 1:5])
```

# Model Performance Comparison

# The Resamples Function

The "resamples()"  function checks that the models are comparable and that
they used the same training scheme ("train_control" configuration).
To do this, after the models are trained, they are added to a list and we
pass this list of models as an argument to the resamples() function in R.

```{r Your 51st Code Chunk}
sapply(lapply(irrigation_scheduling_imputed, unique), length)
train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
```

### CART

```{r Your 52nd Code Chunk}
set.seed(7)
class_model_cart <- train(class ~ ., data = irrigation_scheduling_imputed,
                         method = "rpart", trControl = train_control)

```

### KNN 
```{r Your 53rd Code Chunk}
set.seed(7)
class_model_knn <- train(class ~ ., data = irrigation_scheduling_imputed,
                        method = "knn", trControl = train_control)
```

### SVM
```{r Your 54th Code Chunk}
set.seed(7)
class_model_svm <- train(class ~ ., data = irrigation_scheduling_imputed,
                        method = "svmRadial", trControl = train_control)
```

### Random Forest
```{r Your 55th Code Chunk}
set.seed(7)
class_model_rf <- train(class ~ ., data = irrigation_scheduling_imputed,
                       method = "rf", trControl = train_control)
```

## Call the `resamples` Function

```{r Your 56th Code Chunk}
results <- resamples(list( CART = class_model_cart,
                          KNN = class_model_knn, SVM = class_model_svm,
                          RF = class_model_rf))
```

# Display the Results


## Table Summary
Is the simplest comparison. It creates a table with one model per row
and its corresponding evaluation metrics displayed per column.

```{r Your 57th Code Chunk}
summary(results)
```

## Dot Plots
They show both the mean estimated accuracy as well as the confidence
interval.

```{r Your 58th Code Chunk}
scales <- list(x = list(relation = "free"), y = list(relation = "free"))
dotplot(results, scales = scales)

```


# Hyperparameter Tuning
Involves identifying and applying the best combination
of algorithm parameters

#Train the Model

The "mtry" parameter 
Number of variables randomly sampled as candidates at each split.

```{r Your 59th Code Chunk}
seed <- 7
metric <- "Accuracy"

train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(seed)
mtry <- sqrt(ncol(irrigation_scheduling_imputed))
tunegrid <- expand.grid(.mtry = mtry)
irrigation_model_default_rf <- train(class ~ ., data = irrigation_scheduling_imputed, method = "rf",
                                metric = metric,
                                # enables us to maintain mtry at a constant
                                tuneGrid = tunegrid,
                                trControl = train_control)
print(irrigation_model_default_rf)


```

# Apply a "Random Search" to identify the best parameter value 

A random search is good if we are unsure of what the value might be and
we want to overcome any biases we may have for setting the parameter value.
```{r Your 60th Code Chunk}
train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 3,
                              search = "random")
set.seed(seed)
mtry <- sqrt(ncol(irrigation_scheduling_imputed))

irrigation_model_random_search_rf <- train(class ~ ., data = irrigation_scheduling_imputed, method = "rf",
                                      metric = metric,
                                      # enables us to randomly search 12 options
                                      # for the value of mtry
                                      tuneLength = 12,
                                      trControl = train_control)

print(irrigation_model_random_search_rf)
plot(irrigation_model_random_search_rf)

```

# Ensemble Methods

# Bagging 

# Example of Bagging algorithms

```{r Your 61st Code Chunk}
train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
seed <- 7
metric <- "Accuracy"

```

## Bagged CART 
```{r Your 62nd Code Chunk}
set.seed(seed)
irrigation_model_bagged_cart <- train(class ~ ., data = irrigation_scheduling_imputed, method = "treebag",
                               metric = metric,
                               trControl = train_control)

```

## Random Forest

```{r Your 63rd Code Chunk}
set.seed(seed)
irrigation_model_rf <- train(class ~ ., data =irrigation_scheduling_imputed, method = "rf",
                      metric = metric, trControl = train_control)

```

# Summarize results

```{r Your 64th Code Chunk}
bagging_results <-
  resamples(list("Bagged Decision Tree" = irrigation_model_bagged_cart,
                 "Random Forest" = irrigation_model_rf))

summary(bagging_results)
dotplot(bagging_results)

```

# Plumber API

# Save and Load your Model
Saved models can be loaded by calling the `readRDS()` function

```{r Your 65th Code Chunk}
saveRDS(IrrigationScheduling_caret_model_lda,
"../models/IrrigationScheduling_caret_model_lda.rds")

```

# The saved model can then be loaded later as follows:

```{r Your 66th Code Chunk}
loaded_irrigation_model_lda <- readRDS("../models/IrrigationScheduling_caret_model_lda.rds")
print(loaded_irrigation_model_lda)

predictions_with_loaded_model <-
  predict(loaded_irrigation_model_lda, newdata = IrrigationScheduling_test)



```

# Consume PlumberAPIOutput

# Enclose everything in a function 

```{r Your 67th Code Chunk}
get_class_predictions <-
  function(arg_id, arg_temperature, arg_pressure, arg_altitude,
           arg_soilmiosture, arg_note, arg_status) {
    base_url <- "http://127.0.0.1:5022/irrigation"
    
    params <- list(arg_id = arg_id, arg_temperature = arg_temperature, arg_pressure = arg_pressure, arg_altitude = arg_altitude,
                   arg_soilmiosture = arg_soilmiosture, arg_note = arg_note, arg_status = arg_status)
    
    query_url <- modify_url(url = base_url, query = params)
    
    model_prediction <- GET(query_url)
    
    model_prediction_raw <- content(model_prediction, as = "text",
                                    encoding = "utf-8")
    
    jsonlite::fromJSON(model_prediction_raw)
  }



```

































